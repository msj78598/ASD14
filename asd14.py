import os
import json
import pandas as pd
import numpy as np
import joblib
import streamlit as st
import tensorflow as tf
import matplotlib.pyplot as plt
import io
import urllib.request
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import StackingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# ===============================
# ğŸ”¹ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ===============================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  
MODELS_DIR = os.path.join(BASE_DIR, "models")
DATA_DIR = BASE_DIR  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯
os.makedirs(MODELS_DIR, exist_ok=True)  # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§

# ğŸ”¹ Ø±ÙˆØ§Ø¨Ø· Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ GitHub
GITHUB_REPO = "https://raw.githubusercontent.com/msj78598/ASD14/main/"  # ØºÙŠÙ‘Ø± Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· Ù…Ø³ØªÙˆØ¯Ø¹Ùƒ
MODEL_FILES = {
    "autoencoder_model.keras": os.path.join(MODELS_DIR, "autoencoder_model.keras"),
    "xgboost_model.pkl": os.path.join(MODELS_DIR, "xgboost_model.pkl"),
    "lightgbm_model.pkl": os.path.join(MODELS_DIR, "lightgbm_model.pkl"),
    "stacked_model.pkl": os.path.join(MODELS_DIR, "stacked_model.pkl"),
}

# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
def download_model_files():
    for file_name, file_path in MODEL_FILES.items():
        if not os.path.exists(file_path):
            url = GITHUB_REPO + file_name
            try:
                urllib.request.urlretrieve(url, file_path)
                print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {file_name} Ø¨Ù†Ø¬Ø§Ø­!")
            except Exception as e:
                print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ {file_name}: {e}")

# ğŸ”¹ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
download_model_files()

# ===============================
# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµÙ†ÙØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
# ===============================
train_data_path = os.path.join(DATA_DIR, "final_classified_loss_with_reasons_60_percent_ordered.xlsx")

if not os.path.exists(train_data_path):
    st.error(f"âŒ Ø®Ø·Ø£: Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯! ØªØ£ÙƒØ¯ Ù…Ù† Ø±ÙØ¹Ù‡: {train_data_path}")
    st.stop()

df = pd.read_excel(train_data_path)
df.fillna(df.select_dtypes(include=[np.number]).mean(), inplace=True)
df["Loss_Status"] = df["Loss_Status"].apply(lambda x: 1 if x == "Loss" else 0)

features = ["V1", "V2", "V3", "A1", "A2", "A3"]
X = df[features].values
y = df["Loss_Status"].values

# ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ===============================
# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©
# ===============================
try:
    autoencoder = load_model(MODEL_FILES["autoencoder_model.keras"], compile=False)
    xgb = joblib.load(MODEL_FILES["xgboost_model.pkl"])
    lgbm = joblib.load(MODEL_FILES["lightgbm_model.pkl"])
    stacked_model = joblib.load(MODEL_FILES["stacked_model.pkl"])
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
except Exception as e:
    st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
    st.stop()

# ===============================
# ğŸ”¹ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ Ù„Ù„Ù€ Autoencoder
# ===============================
reconstructions = autoencoder.predict(X_scaled)
mse = np.mean(np.square(reconstructions - X_scaled), axis=1)
threshold = np.percentile(mse, 95)  # Ø§Ù„Ø¹ØªØ¨Ø© Ø¹Ù†Ø¯ 95%

# ===============================
# ğŸ”¹ ØªØµÙ…ÙŠÙ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# ===============================
st.set_page_config(page_title="Ù†Ø¸Ø§Ù… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ§Ù‚Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ", page_icon="âš¡", layout="wide")
st.title("âš¡ Ù†Ø¸Ø§Ù… Ø§ÙƒØªØ´Ø§Ù Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙØ§Ù‚Ø¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©")
st.markdown("### ğŸ¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù‡Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø­Ù…Ø§Ù„ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù„ÙƒØ´Ù Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙØ§Ù‚Ø¯")
st.markdown("---")

st.subheader("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªØ­Ù„ÙŠÙ„Ù‡")
template_file = os.path.join(DATA_DIR, "The_data_frame_file_to_be_analyzed.xlsx")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
if os.path.exists(template_file):
    st.download_button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", open(template_file, "rb"), file_name="The_data_frame_file_to_be_analyzed.xlsx")

uploaded_file = st.file_uploader("ğŸ”¼ Ø±ÙØ¹ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø­Ù…Ø§Ù„")
if uploaded_file:
    df_test = pd.read_excel(uploaded_file)
    X_test = df_test[features].values
    X_test_scaled = scaler.transform(X_test)

    # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Autoencoder
    reconstructions = autoencoder.predict(X_test_scaled)
    mse_test = np.mean(np.square(reconstructions - X_test_scaled), axis=1)
    anomalies = mse_test > threshold

    # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    xgb_preds = xgb.predict(X_test_scaled)
    lgbm_preds = lgbm.predict(X_test_scaled)
    stacked_preds = stacked_model.predict(X_test_scaled)

    # ØªÙØ³ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    def explain_loss(row):
        if row["Stacked_Prediction"] == 1:
            return "âš  Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØªÙÙ‚ Ø¹Ù„Ù‰ Ø£Ù† Ù‡Ø°Ù‡ Ø­Ø§Ù„Ø© ÙØ§Ù‚Ø¯ Ø´Ø¨Ù‡ Ù…Ø¤ÙƒØ¯Ø© Ø¨Ø³Ø¨Ø¨ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶ ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù…Ù„ ÙˆØ§Ù„Ø¬Ù‡Ø¯."
        elif row["Anomaly"]:
            return "âš  Ù…Ø­ØªÙ…Ù„Ø© Ø¨Ø³Ø¨Ø¨ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù…Ù„ ÙˆØ§Ù„ØªÙˆØªØ±."
        return "âœ” Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙØ§Ù‚Ø¯ Ø¸Ø§Ù‡Ø±."

    # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    df_test["Anomaly"] = anomalies
    df_test["XGB_Prediction"] = xgb_preds
    df_test["LGBM_Prediction"] = lgbm_preds
    df_test["Stacked_Prediction"] = stacked_preds
    df_test["Loss_Explanation"] = df_test.apply(explain_loss, axis=1)

    # ØªØµÙÙŠØ© Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
    high_priority_cases = df_test[(df_test["Anomaly"]) &
                                  (df_test["XGB_Prediction"] == 1) &
                                  (df_test["LGBM_Prediction"] == 1) &
                                  (df_test["Stacked_Prediction"] == 1)]

    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    st.write(f"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØµÙ†ÙØ© ÙƒÙØ§Ù‚Ø¯: {len(df_test[df_test['Anomaly']])}")
    st.write(f"ğŸš¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©: {len(high_priority_cases)}")

    # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.subheader("ğŸ” ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©")
    st.dataframe(high_priority_cases[["V1", "V2", "V3", "A1", "A2", "A3", "Loss_Explanation"]])

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    excel_buffer = io.BytesIO()
    with pd.ExcelWriter(excel_buffer, engine="xlsxwriter") as writer:
        high_priority_cases.to_excel(writer, index=False, sheet_name="High Priority Losses")
        df_test.to_excel(writer, index=False, sheet_name="All Predicted Losses")
        writer.close()
    excel_buffer.seek(0)

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    st.download_button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙØ§Ù‚Ø¯ Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©", data=excel_buffer, file_name="High_Priority_Losses.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    st.download_button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙØ§Ù‚Ø¯ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©", data=excel_buffer, file_name="Predicted_Losses.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

st.markdown("---")
st.markdown("ğŸ‘¨â€ğŸ’» **ØªØ·ÙˆÙŠØ± :** Ù…Ø´Ù‡ÙˆØ± Ø§Ù„Ø¹Ø¨Ø§Ø³ | 00966553339838 | ")
